# Cal-QL (Calibrated Conservative Q-Learning)

## ğŸ“Œ èƒŒæ™¯
- **Offline RL** é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š
  1. **Overestimation**ï¼šOOD (out-of-distribution) åŠ¨ä½œçš„ Q å€¼å¯èƒ½è¢«é«˜ä¼°ã€‚
  2. **Over-pessimism**ï¼šä¿å®ˆæ–¹æ³• (å¦‚ CQL) è™½èƒ½é˜²é«˜ä¼°ï¼Œä½†å¯èƒ½è¿‡åº¦æ‚²è§‚ï¼Œå¯¼è‡´ç­–ç•¥é€€æ­¥ (unlearning)ã€‚

- **Cal-QL ç›®æ ‡**ï¼šåœ¨ä¿è¯ä¸é«˜ä¼°çš„åŒæ—¶ï¼Œé¿å…è¿‡åº¦æ‚²è§‚ã€‚
- å¦‚æœæˆ‘ä»¬è¿‡äºæ‚²è§‚ï¼Œé‚£ä¹ˆåœ¨online learningä¸­å­¦ä¹ åˆ°çš„ä¸€äº›å®é™…ä¸Šä¸å¥½çš„åŠ¨ä½œçš„Q-valueä¼šè¡¨ç°å‡ºæ¥æ›´é«˜ï¼Œ

---

## âš™ï¸ æ ¸å¿ƒæ€æƒ³
- å¼•å…¥ä¸€ä¸ª **å‚è€ƒç­–ç•¥ Î¼ (reference policy)**ï¼Œé€šå¸¸é€‰ç”¨è¡Œä¸ºç­–ç•¥ (behavior policy)ã€‚
- å¸Œæœ›å­¦åˆ°çš„ Q å€¼æ»¡è¶³ï¼š
  $$
  V^\mu(s) \leq \mathbb{E}_{a \sim \pi}[Q_\theta(s,a)] \leq Q^\pi_{\text{true}}(s,a)
  $$
  å³ï¼š**ä¸‹ç•Œæ˜¯å‚è€ƒç­–ç•¥ï¼Œä¸Šç•Œæ˜¯çœŸå®ç­–ç•¥**ã€‚

- **å…³é”®æœºåˆ¶**ï¼š
  - å½“ $Q_\theta(s,a) > V^\mu(s)$ï¼šåº”ç”¨ **Conservative Term** å‹ä½ OOD åŠ¨ä½œ â†’ é˜²æ­¢é«˜ä¼°ã€‚
  - å½“ $Q_\theta(s,a) \leq V^\mu(s)$ï¼šä¸å†å‹ä½ï¼Œä¿æŒä¸‹ç•Œ â†’ ==é˜²æ­¢è¿‡åº¦æ‚²è§‚ã€‚==

---

## ğŸ“ Calibrated Regularizer
- æ ‡å‡† CQL æ­£åˆ™é¡¹ï¼š
  $$
  R(\theta) = \mathbb{E}_{s\sim D, a \sim \pi}[Q_\theta(s,a)] - \mathbb{E}_{s,a \sim D}[Q_\theta(s,a)]
  $$

- **Cal-QL ä¿®æ”¹å**ï¼š
  $$
  R(\theta) = \mathbb{E}_{s\sim D, a \sim \pi}[\max(Q_\theta(s,a), V^\mu(s))] - \mathbb{E}_{s,a \sim D}[Q_\theta(s,a)]
  $$

- ç›´è§‚ç†è§£ï¼š
  - `max` æ“ä½œåœ¨ $Q < V^\mu$ æ—¶åˆ‡æ–­æ¢¯åº¦ï¼Œä¸å†å¾€ä¸‹å‹ã€‚
  - åªåœ¨ Q é«˜äºå‚è€ƒç­–ç•¥æ—¶æ‰å‘æŒ¥ CQL ä¿å®ˆä½œç”¨ã€‚
**å­˜åœ¨æ­£åˆ™é¡¹**ä¿è¯äº†æˆ‘ä»¬å¯ä»¥é˜²æ­¢Overestimation
**æ­£åˆ™é¡¹ä¸­**å­˜åœ¨æˆªæ–­æœºåˆ¶ï¼Œä¿è¯äº†æˆ‘ä»¬ä¸ä¼šè¿‡åº¦å‹ä½è¿™é‡Œçš„Qå€¼å¯¼è‡´Unlearningç°è±¡ã€‚
é€‰å®šä¸€ä¸ªReference Policyï¼Œä½¿å¾—æˆ‘ä»¬ä¸ä¼šä½äºè¿™ä¸ªReference Policyå¯¹åº”çš„Qå€¼ã€‚

---

## ğŸ“ˆ ç†è®ºåˆ†æ (Section 6)
- **Cumulative Regret** å¯åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼š
  1. Miscalibration (Q ä½äºæœ€ä¼˜ â†’ æ ¡å‡†æ§åˆ¶)
  2. Overestimation (Q é«˜äºçœŸå® â†’ ä¿å®ˆæ§åˆ¶)

- **å®šç†ç»“æœ**ï¼š
  - å¦‚æœå‚è€ƒç­–ç•¥ Î¼ æ¥è¿‘æœ€ä¼˜ â†’ Cal-QL çš„ regret bound æ˜æ˜¾ä¼˜äº Hybrid RLã€‚
  - å¦‚æœ Î¼ å¾ˆå·® â†’ Cal-QL è‡³å°‘ä¸æ¯” Hybrid RL å·®ã€‚

---

## ğŸ”¬ å®éªŒ (Section 7)
- **é«˜ update-to-data ratio (UTD)** åœºæ™¯ï¼š
  - æ™®é€š Q-learning â†’ OOD å‘æ•£ã€‚
  - CQL â†’ è¿‡åº¦æ‚²è§‚ï¼Œæ€§èƒ½é€€åŒ–ã€‚
  - Cal-QL â†’ ç¨³å®šåœ¨å‚è€ƒå€¼å’ŒçœŸå®å€¼ä¹‹é—´ï¼Œæ€§èƒ½æ›´å¥½ã€‚
- å®éªŒè¯æ˜ Cal-QL åœ¨æç«¯è®­ç»ƒæ¡ä»¶ä¸‹ä»ç„¶é²æ£’ã€‚

---

## âœ… æ€»ç»“
- **TD Error**ï¼šä¿è¯ Q æ”¶æ•›åˆ°çœŸå®å€¼ã€‚
- **Conservative Term**ï¼šé™åˆ¶é«˜ä¼°ã€‚
- **Calibration**ï¼šæä¾›å‚è€ƒç­–ç•¥ä¸‹ç•Œï¼Œé¿å…è¿‡åº¦æ‚²è§‚ã€‚
- **æœ€ç»ˆæ•ˆæœ**ï¼šå­¦åˆ°çš„ Q å§‹ç»ˆå¤¹åœ¨å‚è€ƒç­–ç•¥å’ŒçœŸå®ç­–ç•¥ä¹‹é—´ â†’ ç¨³å®šé«˜æ•ˆã€‚
